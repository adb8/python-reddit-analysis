{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import Counter\n","from datetime import datetime\n","import random\n","import re\n","from gensim import corpora, models\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","import numpy as np\n","import pandas as pd\n","import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","import requests\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import LinearSVC\n","from wordcloud import WordCloud\n","import os\n","import praw\n","import io\n","from dotenv import load_dotenv\n","load_dotenv()"]},{"cell_type":"markdown","metadata":{},"source":["Method 1: Scrape Reddit using Reddit's public API endpoint"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%script false\n","url = 'https://raw.githubusercontent.com/chapmanjacobd/reddit_mining/main/top_text_subreddits.csv'\n","response = requests.get(url)\n","subreddits_df = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n","count = 0\n","save_to = 'data/posts.csv'\n","if not os.path.exists('data'):\n","    os.makedirs('data')\n","\n","for index, row in subreddits_df.iterrows():\n","    subreddit = row['subreddit']\n","    headers = {'User-agent': 'Mozilla/5.0'}\n","    after, limit, posts_data, new_count = None, 100, [], 0\n","    try:\n","        for _ in range(5):\n","            url = f'https://www.reddit.com/r/{subreddit}/hot.json'\n","            params = {'limit': limit, 'after': after, 't': 'year'}\n","            response = requests.get(url, headers=headers, params=params)\n","            data = json.loads(response.text)['data']\n","            posts = data['children']\n","            after = data['after']\n","            count += len(posts)\n","            new_count += len(posts)\n","            posts_data_fetched = [[post['data']['author'], post['data']['title'], post['data']['selftext'], post['data']['created_utc'], subreddit] for post in posts]\n","            posts_data.extend(posts_data_fetched)\n","    except:\n","        print(f'Error collecting posts from {subreddit}')\n","        continue\n","    print(f'{count} total posts collected, {new_count} posts collected from {subreddit}')\n","    posts_df = pd.DataFrame(posts_data, columns=['author', 'title', 'selftext', 'datetime', 'subreddit'])\n","    posts_df.to_csv(save_to, mode='a', header=False, index=False)"]},{"cell_type":"markdown","metadata":{},"source":["Method 2: Scrape Reddit using PRAW"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%script false\n","client_id = os.getenv('CLIENT_ID')\n","client_secret = os.getenv('CLIENT_SECRET')\n","user_agent = os.getenv('USER_AGENT')\n","reddit = praw.Reddit(\n","    client_id=client_id,\n","    client_secret=client_secret,\n","    user_agent=user_agent\n",")\n","\n","url = 'https://raw.githubusercontent.com/chapmanjacobd/reddit_mining/main/top_text_subreddits.csv'\n","response = requests.get(url)\n","subreddits_df = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n","count, added_count, limit = 0, 0, None\n","columns = ['author', 'title', 'selftext', 'created_utc', 'subreddit', 'upvote_ratio', 'score', 'num_comments', 'over_18']\n","posts_parsed, new_posts, posts_df = [], [], pd.DataFrame(columns=columns)\n","save_to = 'data/posts.csv'\n","if not os.path.exists('data'):\n","    os.makedirs('data')\n","    \n","continue_index = 0\n","if os.path.exists(save_to) and os.path.getsize(save_to) > 0:\n","    try:\n","        posts_df = pd.read_csv(save_to)\n","        last_row = posts_df.iloc[-1]\n","        last_subreddit = last_row['subreddit']\n","        continue_index = subreddits_df[subreddits_df['subreddit'] == last_subreddit].index[0] + 1\n","        print(f'Previously scraped posts found, continuing from index {continue_index} in subreddit list')\n","        count = len(posts_df)\n","    except:\n","        print('Error reading posts.csv')\n","        os.exit(0)\n","\n","for index, row in subreddits_df.iterrows():\n","    if index < continue_index:\n","        continue\n","    try:\n","        subreddit = reddit.subreddit(row['subreddit'])\n","        new_posts = list(subreddit.new(limit=limit))\n","        count += len(new_posts)\n","        added_count = len(new_posts)\n","        print(f'{count} total posts collected, {added_count} new posts collected from {subreddit}')\n","        posts_parsed = [[post.author, post.title, post.selftext, post.created_utc, post.subreddit, post.upvote_ratio, post.score, post.num_comments, post.over_18] for post in new_posts]\n","        posts_df = pd.DataFrame(posts_parsed, columns=columns)\n","        if os.path.exists(save_to) and os.path.getsize(save_to) > 0:\n","            posts_df.to_csv(save_to, mode='a', header=False, index=False)\n","        else:\n","            posts_df.to_csv(save_to, mode='a', header=True, index=False)\n","    except:\n","        print(f'Error collecting posts from {subreddit}')\n","        continue"]},{"cell_type":"markdown","metadata":{},"source":["Define sample size, merge title and selftext, and filter by text length"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["posts_df = pd.read_csv('posts.csv')\n","sample_size = -1\n","if sample_size > 0:\n","    posts_df = posts_df.sample(sample_size, random_state=42)\n","posts_df['selftext'] = posts_df['selftext'].fillna('')\n","posts_df['title'] = posts_df['title'].fillna('')\n","posts_df['text'] = posts_df['selftext'] + posts_df['title']\n","minimum_text_length = 20\n","\n","before_filtering = posts_df.shape[0]\n","posts_df = posts_df[posts_df.apply(lambda row: not (row['selftext'] == '[removed]' or row['selftext'] == '[deleted]' or row['title'] == '[removed]' or row['title'] == '[deleted]' or len(row['text']) < minimum_text_length), axis=1)]\n","after_filtering = posts_df.shape[0]\n","print(f'Filtered {before_filtering - after_filtering} posts')\n","print(posts_df.shape)\n","print(display(posts_df.head()))"]},{"cell_type":"markdown","metadata":{},"source":["Remove nonalphanumeric chars, links, and whitespace and lowercase all text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["posts_df['cleaned-text'] = posts_df['text'].apply(lambda text: re.sub(r'\\s+', ' ', re.sub(r'[^a-zA-Z_\\s]', '', re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE))).strip().lower())\n","before_filtering = posts_df.shape[0]\n","posts_df = posts_df[posts_df['cleaned-text'].str.strip() != '']\n","posts_df = posts_df.dropna(subset=['cleaned-text'])\n","after_filtering = posts_df.shape[0]\n","print(f'Filtered {before_filtering - after_filtering} posts')\n","print(display(posts_df[['text', 'cleaned-text']].head()))"]},{"cell_type":"markdown","metadata":{},"source":["Remove stopwords using Kaggle and Princeton stopword datasets"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["url1 = \"https://storage.googleapis.com/kagglesdsdata/datasets/1003424/1692967/stopwords.txt?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20240331%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20240331T232544Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=1dbfcfe70fdc7772a2c63eaf9ded9ed6f841f982e31615e846a0c4a935920c54aff228819773d9eb48d5758e2a283d99929cf2449bd43cccf4d4deeeb62445ddcfd1ffa7465afa2dc6dffe074dd0a5f3f442d745c9c7a32dfe71be4be8c678319282033a5334dbf73ed423319dc441b7066cf7c3051e63c5720c5678fbda846c810e69db9bd2378a9103827c21a114dad6aa103e783b05e58cb8206213106708cf4107fa0f1de75b9571c3a8534d4bdb986e9167ee11f04cb537cbbb2ccef67a75e710782c23195e6f68a84c8f0c5cb930d93680b7dd67b1f7093f37ea841319e1554a519e6ee841c5965c8ab8afdd38a7b219714fc3414d0aeeebc56180d8c2\"\n","response = requests.get(url1)\n","stopwords1 = response.text.splitlines()\n","url2 = \"https://algs4.cs.princeton.edu/35applications/stopwords.txt\"\n","response = requests.get(url2)\n","stopwords2 = response.text.splitlines()\n","\n","minimum_word_length = 5\n","posts_df['cleaned-text'] = posts_df['cleaned-text'].apply(lambda sentence: ' '.join([word for word in sentence.split() if word not in stopwords1 and word not in stopwords2 and len(word) >= minimum_word_length]))\n","posts_df = posts_df[posts_df['cleaned-text'].str.strip() != '']\n","posts_df = posts_df.dropna(subset=['cleaned-text'])\n","posts_df['datetime'] = posts_df['created_utc'].apply(lambda x: datetime.utcfromtimestamp(x))\n","\n","columns_to_drop = ['selftext', 'title', 'text', 'created_utc', 'author']\n","posts_df = posts_df.drop(columns=columns_to_drop)\n","if not os.path.exists('data'):\n","    os.makedirs('data')\n","posts_df.to_csv('data/posts_cleaned.csv', index=False)\n","print(display(posts_df[['cleaned-text', 'datetime']].head()))"]},{"cell_type":"markdown","metadata":{},"source":["Create time of day vs post frequency bar graph"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hour_series = posts_df['datetime'].apply(lambda x: x.hour)\n","activity_by_hour = hour_series.value_counts().sort_index()\n","plt.figure(figsize=(10, 6))\n","plt.bar(activity_by_hour.index, activity_by_hour.values)\n","plt.xlabel('Hour of the Day (UTC)')\n","plt.ylabel('Number of Posts')\n","plt.title('Number of Posts by Hour of the Day')\n","plt.xticks(range(0, 24))\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Create wordcloud of most common words and most common bad words"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = ' '.join(posts_df['cleaned-text'])\n","wordcloud = WordCloud(width=800, height=400).generate(text)\n","plt.figure(figsize=(10, 6))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.show()\n","\n","url = \"https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en\"\n","response = requests.get(url)\n","bad_words = response.text.splitlines()\n","words = ' '.join(posts_df['cleaned-text']).split()\n","word_counts = Counter(words)\n","bad_word_counts = {word: count for word, count in word_counts.items() if word in bad_words}\n","\n","wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(bad_word_counts)\n","plt.figure(figsize=(10, 6))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Perform topic modeling on entire dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["number_of_topics = 3\n","minimum_word_length = 5\n","documents = posts_df['cleaned-text'].tolist()\n","texts = [[word for word in document.split() if len(word) > minimum_word_length] for document in documents]\n","texts = [text for text in texts if len(text) > 0]\n","if len(texts) > 0:\n","    dictionary = corpora.Dictionary(texts)\n","    corpus = [dictionary.doc2bow(text) for text in texts]\n","    lda = models.LdaModel(corpus, num_topics=number_of_topics, id2word=dictionary, passes=5)\n","    topics = lda.print_topics(num_words=5)\n","    for topic in topics:\n","        print(topic)\n","    vis_data = gensimvis.prepare(lda, corpus, dictionary)\n","    if not os.path.exists('pyLDAvis'):\n","        os.makedirs('pyLDAvis')\n","    pyLDAvis.save_html(vis_data, 'pyLDAvis/all_lda.html')\n","    pyLDAvis.display(vis_data)\n","else:\n","    print('No documents to analyze')"]},{"cell_type":"markdown","metadata":{},"source":["Perform topic modeling on specific subreddits"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_subreddits = posts_df['subreddit'].unique().tolist()\n","selected_subreddits = random.sample(unique_subreddits, 5)\n","number_of_topics = 3\n","minimum_word_length = 5\n","\n","for subreddit in selected_subreddits:\n","    subreddit_df = posts_df[posts_df['subreddit'] == subreddit]\n","    documents = subreddit_df['subreddit'].tolist()\n","    texts = [[word for word in document.split() if len(word) > minimum_word_length] for document in documents]\n","    texts = [text for text in texts if len(text) > 0]\n","    if len(texts) > 0:\n","        dictionary = corpora.Dictionary(texts)\n","        corpus = [dictionary.doc2bow(text) for text in texts]\n","        lda = models.LdaModel(corpus, num_topics=number_of_topics, id2word=dictionary, passes=5)\n","        topics = lda.print_topics(num_words=5)\n","        print(f'Subreddit: {subreddit}')\n","        print('Topics:')\n","        for topic in topics:\n","            print(topic)\n","        vis_data = gensimvis.prepare(lda, corpus, dictionary)\n","        if not os.path.exists('pyLDAvis'):\n","            os.makedirs('pyLDAvis')\n","        pyLDAvis.save_html(vis_data, f'pyLDAvis/{subreddit}_lda.html')\n","    else:\n","        print(f'No documents to analyze for {subreddit}')"]},{"cell_type":"markdown","metadata":{},"source":["Create time vs posts graphs for specific subreddits"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_subreddits = posts_df['subreddit'].unique().tolist()\n","selected_subreddits = random.sample(unique_subreddits, 5)\n","\n","for subreddit in selected_subreddits:\n","    subreddit_df = posts_df[posts_df['subreddit'] == subreddit].copy()\n","    subreddit_df['datetime'] = pd.to_datetime(subreddit_df['datetime'])\n","    subreddit_df.set_index('datetime', inplace=True)\n","    daily_counts = subreddit_df.resample('D').size()\n","    if len(daily_counts) > 50:\n","        daily_counts = subreddit_df.resample('W').size()\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(daily_counts.index, daily_counts.values, label='Post Count')\n","    plt.scatter(daily_counts.index, daily_counts.values, color='red')\n","    z = np.polyfit(range(len(daily_counts)), daily_counts.values, 1)\n","    p = np.poly1d(z)\n","    plt.plot(daily_counts.index, p(range(len(daily_counts))), \"r--\", label='Trend Line')\n","    plt.title('Post Frequency Over Time for ' + subreddit)\n","    plt.xlabel('Time')\n","    plt.ylabel('Post Count')\n","    plt.legend()\n","    plt.xticks(rotation=45)\n","    plt.ylim(0, daily_counts.max() + 1)\n","    start_date = pd.to_datetime(daily_counts.index.min())\n","    end_date = pd.to_datetime(daily_counts.index.max())\n","    plt.xlim(start_date, end_date)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Perform sentiment analysis by subreddit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('vader_lexicon')\n","sia = SentimentIntensityAnalyzer()\n","posts_df['sentiment_score'] = posts_df['cleaned-text'].apply(lambda text: sia.polarity_scores(text)['compound'])\n","average_scores = posts_df.groupby('subreddit')['sentiment_score'].mean().sort_values()\n","\n","number_subreddits_every = 100\n","num_unique_subreddits = posts_df['subreddit'].nunique()\n","subreddit_every = num_unique_subreddits // number_subreddits_every\n","average_scores_spliced = average_scores[::subreddit_every]\n","plt.figure(figsize=(10, 25))\n","plt.barh(average_scores_spliced.index, average_scores_spliced.values)\n","plt.xlabel('Sentiment Score')\n","plt.ylabel('Subreddit')\n","if subreddit_every == 1:\n","    plt.title('Sentiment Score by Subreddit')\n","elif str(subreddit_every)[-1] == '1':\n","    plt.title(f'Sentiment Score by Every {subreddit_every}st Subreddit')\n","elif str(subreddit_every)[-1] == '2':\n","    plt.title(f'Sentiment Score by Every {subreddit_every}nd Subreddit')\n","elif str(subreddit_every)[-1] == '3':\n","    plt.title(f'Sentiment Score by Every {subreddit_every}rd Subreddit')\n","else:   \n","    plt.title(f'Sentiment Score by Every {subreddit_every}th Subreddit')\n","plt.xticks(rotation=45)\n","plt.xlim(-1, 1)\n","plt.show()\n","\n","number_of_subreddits_highest = 100\n","average_scores_highest = average_scores.nlargest(number_of_subreddits_highest).sort_values()\n","plt.figure(figsize=(10, 25))\n","plt.barh(average_scores_highest.index, average_scores_highest.values)\n","plt.xlabel('Sentiment Score')\n","plt.ylabel('Subreddit')\n","plt.title(f'Sentiment Score by Highest {number_of_subreddits_highest} Scoring Subreddits')\n","plt.xticks(rotation=45)\n","plt.xlim(-1, 1)\n","plt.show()\n","\n","number_of_subreddits_lowest = 100\n","average_scores_lowest = average_scores.nsmallest(number_of_subreddits_lowest).sort_values()[::-1]\n","plt.figure(figsize=(10, 25))\n","plt.barh(average_scores_lowest.index, average_scores_lowest.values)\n","plt.xlabel('Sentiment Score')\n","plt.ylabel('Subreddit')\n","plt.title(f'Sentiment Score by Lowest {number_of_subreddits_lowest} Scoring Subreddits')\n","plt.xticks(rotation=45)\n","plt.xlim(-1, 1)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Create subreddit prediction model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(posts_df['cleaned-text'], posts_df['subreddit'], test_size=0.2, random_state=42)\n","text_clf = Pipeline([\n","    ('tfidf', TfidfVectorizer()),\n","    ('clf', LinearSVC()),\n","])\n","text_clf.fit(X_train, y_train)\n","\n","predictions = text_clf.predict(X_test)\n","accuracy = accuracy_score(y_test, predictions)\n","print(f\"Model Accuracy: {accuracy}\")\n","report = classification_report(y_test, predictions)\n","print(report)\n","\n","custom_text = \"I am a huge fan of the show Friends. I have watched every episode multiple times and I can't get enough of it. I love the characters and the storylines. I think it is one of the best TV shows ever made.\"\n","prediction = text_clf.predict([custom_text])\n","print(f\"The predicted subreddit for the given post is: {prediction[0]}\")"]}],"metadata":{"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":2}
